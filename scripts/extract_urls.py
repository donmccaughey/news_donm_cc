#!/usr/bin/env python3
# Generated by Juni Pro

import argparse
import json
import os
import sys
from collections import Counter


def parse_options():
    arg_parser = argparse.ArgumentParser(description='Extract and sort URLs from news.json.')
    arg_parser.add_argument('--news-json', dest='news_json',
                            default=os.path.join('tmp', 'news.json'),
                            help='path to the news.json file (default: tmp/news.json)')
    arg_parser.add_argument('--dups-only', dest='dups_only',
                            action='store_true',
                            help='show only duplicate URLs')
    return arg_parser.parse_args()


def extract_urls(news_data):
    """Extract URLs from the items.sources.url field in the news data."""
    url_counter = Counter()
    for item in news_data.get('items', []):
        for source in item.get('sources', []):
            if 'url' in source:
                url_counter[source['url']] += 1
    return url_counter


def main():
    options = parse_options()

    try:
        with open(options.news_json, 'r') as f:
            news_data = json.load(f)
    except FileNotFoundError:
        sys.stderr.write(f"Error: File '{options.news_json}' not found.\n")
        sys.exit(1)
    except json.JSONDecodeError:
        sys.stderr.write(f"Error: File '{options.news_json}' is not valid JSON.\n")
        sys.exit(1)

    url_counter = extract_urls(news_data)

    if options.dups_only:
        # Show only URLs that appear more than once
        duplicate_urls = [url for url, count in url_counter.items() if count > 1]
        for url in sorted(duplicate_urls):
            print(url)
    else:
        # Show all URLs sorted
        for url in sorted(url_counter.keys()):
            print(url)


if __name__ == '__main__':
    main()
